{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de0c4a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.6.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from ollama) (2.12.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from httpx>=0.27->ollama) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from httpx>=0.27->ollama) (2023.7.22)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from httpx>=0.27->ollama) (3.4)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.0)\n",
      "Downloading ollama-0.6.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0705ca62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n",
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='llama3.2:3b', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'hello',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])\n",
    "\n",
    "print(response.message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f320dc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Using cached tf_keras-2.20.1-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow<2.21,>=2.20 (from tf-keras)\n",
      "  Using cached tensorflow-2.20.0-cp311-cp311-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (6.33.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (80.3.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (4.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.64.1)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.12.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (1.26.1)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (3.11.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorflow<2.21,>=2.20->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.21,>=2.20->tf-keras) (2023.7.22)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.6)\n",
      "Requirement already satisfied: pillow in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (11.2.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.21,>=2.20->tf-keras) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.11.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow<2.21,>=2.20->tf-keras) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\burak\\desktop\\kodlama\\pyhton\\python ders\\python ders\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow<2.21,>=2.20->tf-keras) (0.1.2)\n",
      "Using cached tf_keras-2.20.1-py3-none-any.whl (1.7 MB)\n",
      "Using cached tensorflow-2.20.0-cp311-cp311-win_amd64.whl (331.8 MB)\n",
      "Installing collected packages: tensorflow, tf-keras\n",
      "\n",
      "  Attempting uninstall: tensorflow\n",
      "\n",
      "    Found existing installation: tensorflow 2.16.1\n",
      "\n",
      "    Uninstalling tensorflow-2.16.1:\n",
      "\n",
      "      Successfully uninstalled tensorflow-2.16.1\n",
      "\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   ---------------------------------------- 0/2 [tensorflow]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   -------------------- ------------------- 1/2 [tf-keras]\n",
      "   ---------------------------------------- 2/2 [tf-keras]\n",
      "\n",
      "Successfully installed tensorflow-2.20.0 tf-keras-2.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613379f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Embedding modeli ykleniyor: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Metin karakter uzunluu: 26147\n",
      "Embedding boyutu: (384,)\n",
      "tbk_embedding.npy kaydedildi!\n"
     ]
    }
   ],
   "source": [
    "# embed_turkish_text.py\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "TEXT_PATH = \"Data\\Kira_Mevzuat_TBK.txt\"   # senin dosyan\n",
    "\n",
    "def load_text(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def main():\n",
    "    print(\"Embedding modeli ykleniyor:\", MODEL_NAME)\n",
    "    model = SentenceTransformer(MODEL_NAME)  # CPU'da alr\n",
    "\n",
    "    # metni oku\n",
    "    text = load_text(TEXT_PATH)\n",
    "    print(\"Metin karakter uzunluu:\", len(text))\n",
    "\n",
    "    # metni paralara ayrmak yerine ilk adm olarak tek embedding karyoruz\n",
    "    embedding = model.encode(text, convert_to_numpy=True)\n",
    "\n",
    "    print(\"Embedding boyutu:\", embedding.shape)\n",
    "\n",
    "    # Kaydet\n",
    "    np.save(\"tbk_embedding.npy\", embedding)\n",
    "    print(\"tbk_embedding.npy kaydedildi!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9f3f301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam chunk says: 7\n",
      "\n",
      "--- CHUNK 0 ---\n",
      "DRDNC BLM Kira Szlemesi BRNC AYIRIM Genel Hkmler A. Tanm MADDE 299- Kira szlemesi, kiraya verenin bir eyin kullanlmasn veya kullanmayla birlikte ondan yararlanlmasn kiracya brakmay, kiracnn da buna karlk kararlatrlan kira bedelini demeyi stlendii szlemedir. B. Kira sresi MADDE 300- Kira szlemesi, belirli ve belirli olmayan bir sre iin yaplabilir. Kararl...\n",
      "\n",
      "--- CHUNK 1 ---\n",
      "szlemesinin taraf olur. Kamulatrmaya ilikin hkmler sakldr. b. nc kiinin snrl ayni hak sahibi olmas MADDE 311- Szlemenin kurulmasndan sonra nc bir kii, kiralanan zerinde kiracnn hakkn etkileyen bir ayni hak sahibi olursa, kiralanann el deitirmesiyle ilgili hkmler kyas yoluyla uygulanr. c. Tapu siciline erh MADDE 312- Tanmaz kiralarnda, szlemeyle kiracn...\n",
      "\n",
      "--- CHUNK 2 ---\n",
      "olmadka, kiralanann eski durumuyla geri verilmesini isteyemez. Kirac, aksine yazl bir anlama yoksa, kiraya verenin rzasyla yapt yenilik ve deiiklikler sebebiyle kiralananda ortaya kan deer artnn karln isteyemez. II. Alt kira ve kullanm hakknn devri MADDE 322- Kirac, kiraya verene zarar verecek bir deiiklie yol amamak kouluyla, kiralanan tamamen veya ksmen bak...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# chunk_text.py\n",
    "def load_text(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def chunk_by_words(text, min_words=200, max_words=500):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current = []\n",
    "\n",
    "    for w in words:\n",
    "        current.append(w)\n",
    "        # chunk doldu mu?\n",
    "        if len(current) >= max_words:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = []\n",
    "\n",
    "    # son kk paray ekle\n",
    "    if len(current) > 0:\n",
    "        if len(current) < min_words:\n",
    "            # son para kkse bir nceki chunk'a ekle\n",
    "            if chunks:\n",
    "                chunks[-1] += \" \" + \" \".join(current)\n",
    "            else:\n",
    "                chunks.append(\" \".join(current))\n",
    "        else:\n",
    "            chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    text = load_text(\"Data\\Kira_Mevzuat_TBK.txt\")\n",
    "    chunks = chunk_by_words(text, min_words=200, max_words=500)\n",
    "\n",
    "    print(f\"Toplam chunk says: {len(chunks)}\\n\")\n",
    "    for i, c in enumerate(chunks[:3]):\n",
    "        print(f\"--- CHUNK {i} ---\\n{c[:400]}...\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7869399e",
   "metadata": {},
   "source": [
    "# Create Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3774ee23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metin ykleniyor...\n",
      "Chunk'lara blnyor...\n",
      "Toplam 7 chunk oluturuldu.\n",
      "Embedding modeli ykleniyor: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
      "Embedding hesaplanyor...\n",
      "lem tamamland!\n",
      "Kaydedilen embedding boyutu: (7, 384)\n",
      "Dosyalar:\n",
      "  tbk_chunks_embeddings.npy\n",
      "  tbk_chunks_metadata.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "\n",
    "def chunk_by_words(text, min_words=200, max_words=500):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    current = []\n",
    "\n",
    "    for w in words:\n",
    "        current.append(w)\n",
    "        if len(current) >= max_words:\n",
    "            chunks.append(\" \".join(current))\n",
    "            current = []\n",
    "\n",
    "    if len(current) > 0:\n",
    "        if len(current) < min_words:\n",
    "            if chunks:\n",
    "                chunks[-1] += \" \" + \" \".join(current)\n",
    "            else:\n",
    "                chunks.append(\" \".join(current))\n",
    "        else:\n",
    "            chunks.append(\" \".join(current))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "# 3) Ana ilem\n",
    "def main():\n",
    "\n",
    "    TEXT_PATH = \"Data\\Kira_Mevzuat_TBK.txt\"\n",
    "\n",
    "    print(\"Metin ykleniyor...\")\n",
    "    with open(TEXT_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    print(\"Chunk'lara blnyor...\")\n",
    "    chunks = chunk_by_words(text, min_words=200, max_words=500)\n",
    "\n",
    "    print(f\"Toplam {len(chunks)} chunk oluturuldu.\")\n",
    "\n",
    "    print(\"Embedding modeli ykleniyor:\", MODEL_NAME)\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "    embeddings = []\n",
    "    metadatas = []\n",
    "\n",
    "    print(\"Embedding hesaplanyor...\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        emb = model.encode(chunk, convert_to_numpy=True)\n",
    "        embeddings.append(emb)\n",
    "        metadatas.append({\n",
    "            \"chunk_id\": i,\n",
    "            \"word_count\": len(chunk.split()),\n",
    "            \"text_preview\": chunk[:200]\n",
    "        })\n",
    "\n",
    "    embeddings = np.vstack(embeddings)\n",
    "\n",
    "    # 4) Kayt\n",
    "    np.save(\"tbk_chunks_embeddings.npy\", embeddings)\n",
    "    with open(\"tbk_chunks_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadatas, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"lem tamamland!\")\n",
    "    print(\"Kaydedilen embedding boyutu:\", embeddings.shape)\n",
    "    print(\"Dosyalar:\")\n",
    "    print(\"  tbk_chunks_embeddings.npy\")\n",
    "    print(\"  tbk_chunks_metadata.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a0b60e",
   "metadata": {},
   "source": [
    "# Basit Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5482de07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/modules.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001B29C9358D0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: d20f1712-ec04-4f0f-a2af-5596bd026bec)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/./modules.json\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/modules.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001B29C966BD0>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: c5653881-259e-4dff-b3e6-af523e73f870)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/main/./modules.json\n",
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# RNEK SORU\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKiraya verenin ayptan sorumluluu nedir?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 47\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43msemantic_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- EN BENZER CHUNK\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAR ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m result:\n",
      "Cell \u001b[1;32mIn[3], line 14\u001b[0m, in \u001b[0;36msemantic_search\u001b[1;34m(query, top_k)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msemantic_search\u001b[39m(query, top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;66;03m# 1) Soru embedding'i\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     query_emb \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(query, convert_to_numpy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# 2) Chunk embedding'lerini ykle\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:309\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data, backend)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mand\u001b[39;00m model_name_or_path\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m basic_transformer_models:\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;66;03m# A model from sentence-transformers\u001b[39;00m\n\u001b[0;32m    308\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[1;32m--> 309\u001b[0m has_modules \u001b[38;5;241m=\u001b[39m \u001b[43mis_sentence_transformer_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    317\u001b[0m     has_modules\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model_type(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    326\u001b[0m ):\n\u001b[0;32m    327\u001b[0m     modules, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[0;32m    328\u001b[0m         model_name_or_path,\n\u001b[0;32m    329\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[0;32m    337\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\sentence_transformers\\util\\file_io.py:53\u001b[0m, in \u001b[0;36mis_sentence_transformer_model\u001b[1;34m(model_name_or_path, token, cache_folder, revision, local_files_only)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_sentence_transformer_model\u001b[39m(\n\u001b[0;32m     33\u001b[0m     model_name_or_path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m     34\u001b[0m     token: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     local_files_only: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     38\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    Checks if the given model name or path corresponds to a SentenceTransformer model.\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m        bool: True if the model is a SentenceTransformer model, False otherwise.\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\n\u001b[1;32m---> 53\u001b[0m         \u001b[43mload_file_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodules.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\sentence_transformers\\util\\file_io.py:96\u001b[0m, in \u001b[0;36mload_file_path\u001b[1;34m(model_name_or_path, filename, subfolder, token, cache_folder, revision, local_files_only)\u001b[0m\n\u001b[0;32m     94\u001b[0m file_path \u001b[38;5;241m=\u001b[39m Path(subfolder, filename)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_posix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msentence-transformers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\huggingface_hub\\file_download.py:1007\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    987\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m    989\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1004\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1005\u001b[0m     )\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\huggingface_hub\\file_download.py:1070\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1066\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n\u001b[0;32m   1068\u001b[0m \u001b[38;5;66;03m# Try to get metadata (etag, commit_hash, url, size) from the server.\u001b[39;00m\n\u001b[0;32m   1069\u001b[0m \u001b[38;5;66;03m# If we can't, a HEAD request error is returned.\u001b[39;00m\n\u001b[1;32m-> 1070\u001b[0m (url_to_download, etag, commit_hash, expected_size, xet_file_data, head_call_error) \u001b[38;5;241m=\u001b[39m \u001b[43m_get_metadata_or_catch_error\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_filename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1083\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;66;03m# etag can be None for several reasons:\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m \u001b[38;5;66;03m# 1. we passed local_files_only.\u001b[39;00m\n\u001b[0;32m   1087\u001b[0m \u001b[38;5;66;03m# 2. we don't have a connection\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[38;5;66;03m# If the specified revision is a commit hash, look inside \"snapshots\".\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;66;03m# If the specified revision is a branch or tag, look inside \"refs\".\u001b[39;00m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_call_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;66;03m# Couldn't make a HEAD call => let's try to find a local file\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\huggingface_hub\\file_download.py:1543\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1542\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1543\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m            \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1546\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[0;32m   1547\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m storage_folder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1548\u001b[0m             \u001b[38;5;66;03m# Cache the non-existence of the file\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\huggingface_hub\\file_download.py:1460\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[0;32m   1457\u001b[0m hf_headers[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccept-Encoding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midentity\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# prevent any compression => we want to know the real size of the file\u001b[39;00m\n\u001b[0;32m   1459\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1460\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1462\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1463\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1469\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m   1471\u001b[0m \u001b[38;5;66;03m# Return\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\huggingface_hub\\file_download.py:283\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# Recursively follow relative redirects\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 283\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m300\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m399\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\huggingface_hub\\file_download.py:306\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[0;32m    305\u001b[0m \u001b[38;5;66;03m# Perform request and return if status_code is not in the retry list.\u001b[39;00m\n\u001b[1;32m--> 306\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mhttp_backoff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m hf_raise_for_status(response)\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\huggingface_hub\\utils\\_http.py:329\u001b[0m, in \u001b[0;36mhttp_backoff\u001b[1;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;66;03m# Sleep for X seconds\u001b[39;00m\n\u001b[0;32m    328\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms [Retry \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnb_tries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m].\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 329\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(sleep_time)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# Update sleep time for next retry\u001b[39;00m\n\u001b[0;32m    332\u001b[0m sleep_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_wait_time, sleep_time \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# semantic_search.py\n",
    "import numpy as np\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from numpy.linalg import norm\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "def semantic_search(query, top_k=3):\n",
    "    # 1) Soru embedding'i\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    query_emb = model.encode(query, convert_to_numpy=True)\n",
    "\n",
    "    # 2) Chunk embedding'lerini ykle\n",
    "    embeddings = np.load(\"tbk_chunks_embeddings.npy\")\n",
    "    with open(\"tbk_chunks_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "        metadata = json.load(f)\n",
    "\n",
    "    # 3) Skor hesapla\n",
    "    scores = []\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        score = cosine_similarity(query_emb, emb)\n",
    "        scores.append((i, score))\n",
    "\n",
    "    # 4) Skora gre srala\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    top = scores[:top_k]\n",
    "\n",
    "    # 5) Sonular gster\n",
    "    results = []\n",
    "    for idx, score in top:\n",
    "        results.append({\n",
    "            \"chunk_id\": idx,\n",
    "            \"score\": float(score),\n",
    "            \"preview\": metadata[idx][\"text_preview\"]\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # RNEK SORU\n",
    "    query = \"Kiraya verenin ayptan sorumluluu nedir?\"\n",
    "    result = semantic_search(query)\n",
    "\n",
    "    print(\"\\n--- EN BENZER CHUNK'LAR ---\\n\")\n",
    "    for r in result:\n",
    "        print(f\"Chunk #{r['chunk_id']} | Score: {r['score']:.4f}\")\n",
    "        print(r[\"preview\"])\n",
    "        print(\"\\n----------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1cf41c",
   "metadata": {},
   "source": [
    "# Llama 3.2:3b and Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35bf8dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- OLLAMA PROMPT ---\n",
      "\n",
      "Aadaki balamdan yararlanarak soruyu yantla. Sadece balamdaki bilgiye dayan. Balamda yoksa \"Bilmiyorum.\" de. BALAM: [CHUNK 2 | score=0.5312] olmadka, kiralanann eski durumuyla geri verilmesini isteyemez. Kirac, aksine yazl bir anlama yoksa, kiraya verenin rzasyla yapt yenilik ve deiiklikler sebebiyle kiralananda ortaya kan [CHUNK 3 | score=0.5269] bildirim sresine uyarak her zaman feshedebilir. Kiraya verenin meslek faaliyeti gerei kiraya verdii ve kiracnn da zel kullanmna yarayan tanr bir maln kiracs, kira szlemesini,  aylk [CHUNK 6 | score=0.5041] edilmesinden sonra, kiraya verene kar, kiralanan belli bir tarihte boaltmay yazl olarak stlendii hlde boaltmamsa kiraya veren, kira szlemesini bu tarihten balayarak bir ay iinde icray SORU: ev sahibi istedii gibi evden kiracy kartabilir mi CEVAP:\n",
      "\n",
      "--- EN BENZER CHUNK'LAR ---\n",
      "Chunk 2 | score=0.5312\n",
      "olmadka, kiralanann eski durumuyla geri verilmesini isteyemez. Kirac, aksine yazl bir anlama yoksa, kiraya verenin rzasyla yapt yenilik ve deiiklikler sebebiyle kiralananda ortaya kan ...\n",
      "----------------------------\n",
      "Chunk 3 | score=0.5269\n",
      "bildirim sresine uyarak her zaman feshedebilir. Kiraya verenin meslek faaliyeti gerei kiraya verdii ve kiracnn da zel kullanmna yarayan tanr bir maln kiracs, kira szlemesini,  aylk...\n",
      "----------------------------\n",
      "Chunk 6 | score=0.5041\n",
      "edilmesinden sonra, kiraya verene kar, kiralanan belli bir tarihte boaltmay yazl olarak stlendii hlde boaltmamsa kiraya veren, kira szlemesini bu tarihten balayarak bir ay iinde icray...\n",
      "----------------------------\n",
      "\n",
      "--- OLLAMA CEVAP ---\n",
      "\n",
      "Hayr. Kirac, aksine yazl bir anlama yoksa, kiraya verenin rzasyla yapt yenilik ve deiiklikler sebebiyle kiralananda ortaya kan bildirim sresine uyarak her zaman feshedebilir.\n"
     ]
    }
   ],
   "source": [
    "# rag_with_ollama.py\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import textwrap\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "EMBED_FILE = \"tbk_chunks_embeddings.npy\"\n",
    "META_FILE = \"tbk_chunks_metadata.json\"\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"   # Ollama'nn endpoint'i\n",
    "OLLAMA_MODEL = \"llama3.2:3b\"   # sen hangisini kullandysan (r: llama3.2:3b)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return float(np.dot(a, b) / (norm(a) * norm(b)))\n",
    "\n",
    "# semantic search\n",
    "def semantic_search(query, top_k=3):\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    q_emb = model.encode(query, convert_to_numpy=True)\n",
    "\n",
    "    embeddings = np.load(EMBED_FILE)\n",
    "    with open(META_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    scores = []\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        score = cosine_similarity(q_emb, emb)\n",
    "        scores.append((i, score))\n",
    "\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    results = []\n",
    "    for idx, score in scores[:top_k]:\n",
    "        results.append({\n",
    "            \"chunk_id\": idx,\n",
    "            \"score\": score,\n",
    "            \"text\": meta[idx][\"text_preview\"]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Ollama'ya balaml prompt gnder\n",
    "def ask_ollama(prompt):\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(OLLAMA_URL, json=payload)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Ollama hatas: {response.text}\")\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "# RAG pipeline\n",
    "def rag_answer(question):\n",
    "    retrieved = semantic_search(question, top_k=3)\n",
    "\n",
    "    # prompt olutur\n",
    "    context_text = \"\"\n",
    "    for r in retrieved:\n",
    "        context_text += f\"[CHUNK {r['chunk_id']} | score={r['score']:.4f}]\\n{r['text']}\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Aadaki balamdan yararlanarak soruyu yantla.\n",
    "Sadece balamdaki bilgiye dayan. Balamda yoksa \"Bilmiyorum.\" de.\n",
    "\n",
    "BALAM:\n",
    "{context_text}\n",
    "\n",
    "SORU: {question}\n",
    "\n",
    "CEVAP:\n",
    "\"\"\"\n",
    "\n",
    "    print(\"\\n--- OLLAMA PROMPT ---\\n\")\n",
    "    print(textwrap.shorten(prompt, width=1200))\n",
    "\n",
    "    answer = ask_ollama(prompt)\n",
    "    return answer, retrieved\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q = input(\"Soru: \").strip()\n",
    "    answer, retrieved = rag_answer(q)\n",
    "\n",
    "    print(\"\\n--- EN BENZER CHUNK'LAR ---\")\n",
    "    for r in retrieved:\n",
    "        print(f\"Chunk {r['chunk_id']} | score={r['score']:.4f}\")\n",
    "        print(r['text'][:300] + \"...\")\n",
    "        print(\"----------------------------\")\n",
    "\n",
    "    print(\"\\n--- OLLAMA CEVAP ---\\n\")\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "713891b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- OLLAMA PROMPT ---\n",
      "\n",
      "Aadaki balamdan yararlanarak soruyu yantla. Sadece balamdaki bilgiye dayan. Balamda yoksa \"Bilmiyorum.\" de. BALAM: [CHUNK 6 | score=0.6160] edilmesinden sonra, kiraya verene kar, kiralanan belli bir tarihte boaltmay yazl olarak stlendii hlde boaltmamsa kiraya veren, kira szlemesini bu tarihten balayarak bir ay iinde icray [CHUNK 2 | score=0.5983] olmadka, kiralanann eski durumuyla geri verilmesini isteyemez. Kirac, aksine yazl bir anlama yoksa, kiraya verenin rzasyla yapt yenilik ve deiiklikler sebebiyle kiralananda ortaya kan [CHUNK 5 | score=0.5955] kiraya veren tarafndan bu sre iinde kira bedelinin artrlacana ilikin olarak kiracya yazl bildirimde bulunulmu olmas kouluyla, izleyen yeni kira dnemi sonuna kadar ald takdirde, mah SORU: kira nedir CEVAP:\n",
      "\n",
      "--- EN BENZER CHUNK'LAR ---\n",
      "Chunk 6 | score=0.6160\n",
      "edilmesinden sonra, kiraya verene kar, kiralanan belli bir tarihte boaltmay yazl olarak stlendii hlde boaltmamsa kiraya veren, kira szlemesini bu tarihten balayarak bir ay iinde icray...\n",
      "----------------------------\n",
      "Chunk 2 | score=0.5983\n",
      "olmadka, kiralanann eski durumuyla geri verilmesini isteyemez. Kirac, aksine yazl bir anlama yoksa, kiraya verenin rzasyla yapt yenilik ve deiiklikler sebebiyle kiralananda ortaya kan ...\n",
      "----------------------------\n",
      "Chunk 5 | score=0.5955\n",
      "kiraya veren tarafndan bu sre iinde kira bedelinin artrlacana ilikin olarak kiracya yazl bildirimde bulunulmu olmas kouluyla, izleyen yeni kira dnemi sonuna kadar ald takdirde, mah...\n",
      "----------------------------\n",
      "\n",
      "--- OLLAMA CEVAP ---\n",
      "\n",
      "Kira, bir binaya veya tanmazn sahipleri tarafndan kiracya (kiralanan) kiraclk yapmas ve kiracya bir bedel (kira bedeli) demesini salamas iin yaplan anlama.\n"
     ]
    }
   ],
   "source": [
    "# rag_with_ollama.py\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import textwrap\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "EMBED_FILE = \"tbk_chunks_embeddings.npy\"\n",
    "META_FILE = \"tbk_chunks_metadata.json\"\n",
    "\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"   # Ollama'nn endpoint'i\n",
    "OLLAMA_MODEL = \"llama3.2:3b\"   # sen hangisini kullandysan (r: llama3.2:3b)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return float(np.dot(a, b) / (norm(a) * norm(b)))\n",
    "\n",
    "# semantic search\n",
    "def semantic_search(query, top_k=3):\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    q_emb = model.encode(query, convert_to_numpy=True)\n",
    "\n",
    "    embeddings = np.load(EMBED_FILE)\n",
    "    with open(META_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    scores = []\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        score = cosine_similarity(q_emb, emb)\n",
    "        scores.append((i, score))\n",
    "\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    results = []\n",
    "    for idx, score in scores[:top_k]:\n",
    "        results.append({\n",
    "            \"chunk_id\": idx,\n",
    "            \"score\": score,\n",
    "            \"text\": meta[idx][\"text_preview\"]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Ollama'ya balaml prompt gnder\n",
    "def ask_ollama(prompt):\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(OLLAMA_URL, json=payload)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Ollama hatas: {response.text}\")\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "# RAG pipeline\n",
    "def rag_answer(question):\n",
    "    retrieved = semantic_search(question, top_k=3)\n",
    "\n",
    "    # prompt olutur\n",
    "    context_text = \"\"\n",
    "    for r in retrieved:\n",
    "        context_text += f\"[CHUNK {r['chunk_id']} | score={r['score']:.4f}]\\n{r['text']}\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Aadaki balamdan yararlanarak soruyu yantla.\n",
    "Sadece balamdaki bilgiye dayan. Balamda yoksa \"Bilmiyorum.\" de.\n",
    "\n",
    "BALAM:\n",
    "{context_text}\n",
    "\n",
    "SORU: {question}\n",
    "\n",
    "CEVAP:\n",
    "\"\"\"\n",
    "\n",
    "    print(\"\\n--- OLLAMA PROMPT ---\\n\")\n",
    "    print(textwrap.shorten(prompt, width=1200))\n",
    "\n",
    "    answer = ask_ollama(prompt)\n",
    "    return answer, retrieved\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q = input(\"Soru: \").strip()\n",
    "    answer, retrieved = rag_answer(q)\n",
    "\n",
    "    print(\"\\n--- EN BENZER CHUNK'LAR ---\")\n",
    "    for r in retrieved:\n",
    "        print(f\"Chunk {r['chunk_id']} | score={r['score']:.4f}\")\n",
    "        print(r['text'][:300] + \"...\")\n",
    "        print(\"----------------------------\")\n",
    "\n",
    "    print(\"\\n--- OLLAMA CEVAP ---\\n\")\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0a120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Fetching 28 files:  21%|       | 6/28 [00:00<00:03,  7.26it/s]'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a2fbec20-c3af-402e-a8f3-32d5c4778ee8)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/onnx/model_quint8_avx2.onnx\n",
      "Retrying in 1s [Retry 1/5].\n",
      "'(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/onnx/model_quint8_avx2.onnx (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027602D89C50>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: d9dd99fe-dfa6-4629-99d4-844ed9be8e66)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/onnx/model_quint8_avx2.onnx\n",
      "Retrying in 2s [Retry 2/5].\n",
      "Fetching 28 files:  32%|      | 9/28 [20:04<51:37, 163.05s/it]  '(MaxRetryError('HTTPSConnectionPool(host=\\'huggingface.co\\', port=443): Max retries exceeded with url: /sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000027602DBB750>: Failed to resolve \\'huggingface.co\\' ([Errno 11001] getaddrinfo failed)\"))'), '(Request ID: 343a1e06-46c9-484a-9873-ad90c2f02cb9)')' thrown while requesting HEAD https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2/resolve/86741b4e3f5cb7765a600d3a3d55a0f6a6cb443d/tokenizer.json\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    local_dir=\"models/paraphrase\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f90ed69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'paraphrase-multilingual-MiniLM-L12-v2'...\n",
      "Filtering content:  11% (2/17)\n",
      "Filtering content:  11% (2/17), 897.14 MiB | 64.45 MiB/s\n",
      "Filtering content:  17% (3/17), 897.14 MiB | 64.45 MiB/s\n",
      "Filtering content:  17% (3/17), 1.31 GiB | 59.30 MiB/s  \n",
      "Filtering content:  23% (4/17), 1.31 GiB | 59.30 MiB/s\n",
      "Filtering content:  23% (4/17), 1.75 GiB | 50.21 MiB/s\n",
      "Filtering content:  29% (5/17), 1.75 GiB | 50.21 MiB/s\n",
      "Filtering content:  29% (5/17), 2.19 GiB | 60.51 MiB/s\n",
      "Filtering content:  35% (6/17), 2.19 GiB | 60.51 MiB/s\n",
      "Filtering content:  35% (6/17), 2.62 GiB | 68.43 MiB/s\n",
      "Filtering content:  41% (7/17), 2.62 GiB | 68.43 MiB/s\n",
      "Filtering content:  47% (8/17), 2.62 GiB | 68.43 MiB/s\n",
      "Filtering content:  52% (9/17), 2.62 GiB | 68.43 MiB/s\n",
      "Filtering content:  58% (10/17), 2.62 GiB | 68.43 MiB/s\n",
      "Filtering content:  64% (11/17), 2.76 GiB | 71.14 MiB/s\n",
      "Filtering content:  64% (11/17), 2.87 GiB | 73.38 MiB/s\n",
      "Filtering content:  70% (12/17), 2.87 GiB | 73.38 MiB/s\n",
      "Filtering content:  70% (12/17), 2.98 GiB | 71.36 MiB/s\n",
      "Filtering content:  76% (13/17), 2.98 GiB | 71.36 MiB/s\n",
      "Filtering content:  76% (13/17), 3.09 GiB | 68.01 MiB/s\n",
      "Filtering content:  82% (14/17), 3.09 GiB | 68.01 MiB/s\n",
      "Filtering content:  82% (14/17), 3.20 GiB | 69.23 MiB/s\n",
      "Filtering content:  88% (15/17), 3.20 GiB | 69.23 MiB/s\n",
      "Filtering content:  88% (15/17), 3.64 GiB | 67.59 MiB/s\n",
      "Filtering content:  94% (16/17), 3.64 GiB | 67.59 MiB/s\n",
      "Filtering content:  94% (16/17), 3.86 GiB | 81.84 MiB/s\n",
      "Filtering content: 100% (17/17), 3.86 GiB | 81.84 MiB/s\n",
      "Filtering content: 100% (17/17), 4.30 GiB | 64.05 MiB/s\n",
      "Filtering content: 100% (17/17), 4.30 GiB | 15.95 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://huggingface.co/sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5df91856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- OLLAMA PROMPT ---\n",
      "\n",
      "Aadaki balamdan yararlanarak soruyu yantla. Sadece balamdaki bilgiye dayan. Balamda yoksa \"Bilmiyorum.\" de. BALAM: [CHUNK 6 | score=0.6160] edilmesinden sonra, kiraya verene kar, kiralanan belli bir tarihte boaltmay yazl olarak stlendii hlde boaltmamsa kiraya veren, kira szlemesini bu tarihten balayarak bir ay iinde icray [CHUNK 2 | score=0.5983] olmadka, kiralanann eski durumuyla geri verilmesini isteyemez. Kirac, aksine yazl bir anlama yoksa, kiraya verenin rzasyla yapt yenilik ve deiiklikler sebebiyle kiralananda ortaya kan [CHUNK 5 | score=0.5955] kiraya veren tarafndan bu sre iinde kira bedelinin artrlacana ilikin olarak kiracya yazl bildirimde bulunulmu olmas kouluyla, izleyen yeni kira dnemi sonuna kadar ald takdirde, mah SORU: kira nedir CEVAP:\n",
      "\n",
      "--- EN BENZER CHUNK'LAR ---\n",
      "Chunk 6 | score=0.6160\n",
      "edilmesinden sonra, kiraya verene kar, kiralanan belli bir tarihte boaltmay yazl olarak stlendii hlde boaltmamsa kiraya veren, kira szlemesini bu tarihten balayarak bir ay iinde icray...\n",
      "----------------------------\n",
      "Chunk 2 | score=0.5983\n",
      "olmadka, kiralanann eski durumuyla geri verilmesini isteyemez. Kirac, aksine yazl bir anlama yoksa, kiraya verenin rzasyla yapt yenilik ve deiiklikler sebebiyle kiralananda ortaya kan ...\n",
      "----------------------------\n",
      "Chunk 5 | score=0.5955\n",
      "kiraya veren tarafndan bu sre iinde kira bedelinin artrlacana ilikin olarak kiracya yazl bildirimde bulunulmu olmas kouluyla, izleyen yeni kira dnemi sonuna kadar ald takdirde, mah...\n",
      "----------------------------\n",
      "\n",
      "--- OLLAMA CEVAP ---\n",
      "\n",
      "Kira, bir binal konutun, bir arsnn veya bir yerin Kirac'ya kiraya verildiinde, o konuttan veya arsadan veya yerde edilen kiras demi olan kiiye ait olan maln, maln sahipleri tarafndan belirlenen time (kira sresi) iinde kiracya ve kiraya verene arasnda yaplan anlama ile dzenlenecek szlemedir.\n"
     ]
    }
   ],
   "source": [
    "# rag_with_ollama.py\n",
    "import json\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import requests\n",
    "import textwrap\n",
    "\n",
    "#  Lokal model yolu (git clone ile indirdiin klasr)\n",
    "MODEL_NAME = \"Models\\paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "\n",
    "# embedding dosyalar\n",
    "EMBED_FILE = \"tbk_chunks_embeddings.npy\"\n",
    "META_FILE = \"tbk_chunks_metadata.json\"\n",
    "\n",
    "# Ollama API\n",
    "OLLAMA_URL = \"http://localhost:11434/api/generate\"\n",
    "OLLAMA_MODEL = \"llama3.2:3b\"\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return float(np.dot(a, b) / (norm(a) * norm(b)))\n",
    "\n",
    "# semantic search\n",
    "def semantic_search(query, top_k=3):\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    q_emb = model.encode(query, convert_to_numpy=True)\n",
    "\n",
    "    embeddings = np.load(EMBED_FILE)\n",
    "    with open(META_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        meta = json.load(f)\n",
    "\n",
    "    scores = []\n",
    "    for i, emb in enumerate(embeddings):\n",
    "        score = cosine_similarity(q_emb, emb)\n",
    "        scores.append((i, score))\n",
    "\n",
    "    scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    results = []\n",
    "    for idx, score in scores[:top_k]:\n",
    "        results.append({\n",
    "            \"chunk_id\": idx,\n",
    "            \"score\": score,\n",
    "            \"text\": meta[idx][\"text_preview\"]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Ollama'ya balaml prompt gnder\n",
    "def ask_ollama(prompt):\n",
    "    payload = {\n",
    "        \"model\": OLLAMA_MODEL,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    response = requests.post(OLLAMA_URL, json=payload)\n",
    "    if response.status_code != 200:\n",
    "        raise Exception(f\"Ollama hatas: {response.text}\")\n",
    "    return response.json()[\"response\"]\n",
    "\n",
    "# RAG pipeline\n",
    "def rag_answer(question):\n",
    "    retrieved = semantic_search(question, top_k=3)\n",
    "\n",
    "    # prompt olutur\n",
    "    context_text = \"\"\n",
    "    for r in retrieved:\n",
    "        context_text += (\n",
    "            f\"[CHUNK {r['chunk_id']} | score={r['score']:.4f}]\\n\"\n",
    "            f\"{r['text']}\\n\\n\"\n",
    "        )\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "Aadaki balamdan yararlanarak soruyu yantla.\n",
    "Sadece balamdaki bilgiye dayan. Balamda yoksa \"Bilmiyorum.\" de.\n",
    "\n",
    "BALAM:\n",
    "{context_text}\n",
    "\n",
    "SORU: {question}\n",
    "\n",
    "CEVAP:\n",
    "\"\"\"\n",
    "\n",
    "    print(\"\\n--- OLLAMA PROMPT ---\\n\")\n",
    "    print(textwrap.shorten(prompt, width=1200))\n",
    "\n",
    "    answer = ask_ollama(prompt)\n",
    "    return answer, retrieved\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    q = input(\"Soru: \").strip()\n",
    "    answer, retrieved = rag_answer(q)\n",
    "\n",
    "    print(\"\\n--- EN BENZER CHUNK'LAR ---\")\n",
    "    for r in retrieved:\n",
    "        print(f\"Chunk {r['chunk_id']} | score={r['score']:.4f}\")\n",
    "        print(r['text'][:300] + \"...\")\n",
    "        print(\"----------------------------\")\n",
    "\n",
    "    print(\"\\n--- OLLAMA CEVAP ---\\n\")\n",
    "    print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe7994",
   "metadata": {},
   "source": [
    "# Daha iyi chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e3b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam 57 chunk oluturuldu (maddelere gre).\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "INPUT_FILE = \"Data/Kira_Mevzuat_TBK.txt\"\n",
    "OUTPUT_JSON = \"tbk_chunks_metadata.json\"\n",
    "OUTPUT_TEXT_DIR = \"chunks/\"\n",
    "\n",
    "import os\n",
    "os.makedirs(OUTPUT_TEXT_DIR, exist_ok=True)\n",
    "\n",
    "def load_text(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def split_by_articles(text):\n",
    "    \n",
    "    parts = re.split(r'(MADDE\\s+\\d+\\-)', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current = \"\"\n",
    "\n",
    "    \n",
    "    for i in range(1, len(parts), 2):\n",
    "        article_title = parts[i].strip()  \n",
    "        article_text = parts[i+1].strip()\n",
    "        full_chunk = f\"{article_title}\\n{article_text}\"\n",
    "        chunks.append(full_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def save_chunks(chunks):\n",
    "    meta = []\n",
    "    for i, ch in enumerate(chunks):\n",
    "        with open(f\"{OUTPUT_TEXT_DIR}/chunk_{i}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(ch)\n",
    "\n",
    "        meta.append({\n",
    "            \"id\": i,\n",
    "            \"text_preview\": ch[:300].replace(\"\\n\", \" \")\n",
    "        })\n",
    "\n",
    "    with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "text = load_text(INPUT_FILE)\n",
    "chunks = split_by_articles(text)\n",
    "save_chunks(chunks)\n",
    "\n",
    "print(f\"Toplam {len(chunks)} chunk oluturuldu (maddelere gre).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053c4603",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\burak\\Desktop\\Kodlama\\Pyhton\\Python Ders\\Python Ders\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Embedding tamamland!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import glob\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "MODEL_NAME = \"Models/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "OUT_EMB = \"tbk_chunks_embeddings.npy\"\n",
    "OUT_META = \"tbk_chunks_metadata.json\"\n",
    "\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "\n",
    "chunk_files = sorted(glob.glob(\"chunks/*.txt\"))\n",
    "\n",
    "embeddings = []\n",
    "meta = []\n",
    "\n",
    "for idx, file in enumerate(chunk_files):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    emb = model.encode(text, convert_to_numpy=True)\n",
    "    embeddings.append(emb)\n",
    "\n",
    "    meta.append({\n",
    "        \"id\": idx,\n",
    "        \"file\": file,\n",
    "        \"text_preview\": text[:300].replace(\"\\n\",\" \")\n",
    "    })\n",
    "\n",
    "np.save(OUT_EMB, np.vstack(embeddings))\n",
    "\n",
    "with open(OUT_META, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Embedding tamamland!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdd6339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python Ders",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
